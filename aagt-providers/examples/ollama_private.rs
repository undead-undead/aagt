//! Ollama provider example - Private local trading agent
//!
//! Run with: cargo run --example ollama_private --features ollama
//!
//! Prerequisites:
//! 1. Install Ollama: https://ollama.ai
//! 2. Pull a model: ollama pull llama3.1:8b
//! 3. Start Ollama server (usually auto-starts)

use aagt_core::prelude::*;
use aagt_providers::ollama::{Ollama, LLAMA_3_1_8B};

#[tokio::main]
async fn main() -> Result<()> {
    // Initialize tracing
    tracing_subscriber::fmt()
        .with_max_level(tracing::Level::INFO)
        .init();

    // Create Ollama provider (local & private!)
    let provider = Ollama::from_env()?;

    // Build a private trading agent
    let agent = Agent::builder(provider)
        .model(LLAMA_3_1_8B)
        .system_prompt(
            "You are a private trading strategy analyst. \
             All conversations are confidential and never leave this machine. \
             Analyze trading strategies for Solana DeFi protocols."
        )
        .max_history_messages(10)
        .build()?;

    println!("ğŸ” Ollama Private Trading Agent");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("Model: {} (Local)", LLAMA_3_1_8B);
    println!("Privacy: 100% - No data leaves your machine");
    println!("Cost: $0 - Unlimited usage");
    println!();

    // Example: Discuss proprietary trading strategy
    let response = agent
        .prompt(
            "I'm developing a MEV arbitrage strategy on Solana. \
             Should I focus on Jupiter swaps or Orca pools? \
             Consider slippage and gas costs."
        )
        .await?;

    println!("ğŸ¤– Agent Response:");
    println!("{}", response);
    println!();
    
    println!("ğŸ’¡ Ollama advantages:");
    println!("   âœ… Complete privacy - protect your alpha");
    println!("   âœ… Zero API costs - unlimited queries");
    println!("   âœ… No rate limits - query as much as needed");
    println!("   âœ… Works offline - no internet required");
    println!();
    
    println!("ğŸ“Š Recommended models for trading:");
    println!("   â€¢ llama3.1:8b   - Fast, balanced");
    println!("   â€¢ llama3.1:70b  - Most capable (needs GPU)");
    println!("   â€¢ mistral:7b    - Good for analysis");
    println!("   â€¢ qwen2.5:7b    - Excellent reasoning");
    println!();
    
    println!("ğŸ› ï¸  Setup tips:");
    println!("   1. ollama pull llama3.1:8b");
    println!("   2. Set OLLAMA_BASE_URL if needed");
    println!("   3. Use GPU for faster inference");

    Ok(())
}
